{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BpU8568GqVm8",
    "outputId": "dd8cfe2a-4f6d-4b31-a180-0aa13c07403c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAJo-Joxjwv_"
   },
   "source": [
    "# Project-2: Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "sOmD2gPPjwv_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from random import randrange\n",
    "from shutil import copyfile\n",
    "\n",
    "import binascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "ymeBIwWlemsc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UE86DWvjwwC"
   },
   "source": [
    "## Execute the follwing two cells to generate your data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "Qt4A2O_v-AdX"
   },
   "outputs": [],
   "source": [
    "n_1=0\n",
    "n_2=0\n",
    "n_3=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "cdEWHCgzjwwD"
   },
   "outputs": [],
   "source": [
    "MY_ID = '692'\n",
    "# MY_ID should be string\n",
    "# Example MY_ID='819'\n",
    "\n",
    "task_dict = {0:'taska',1:'taskb',2:'taskc',3:'taskd',4:'taske'}\n",
    "id_dict = {0:n_1,1:n_2,2:n_3}\n",
    "\n",
    "try:\n",
    "    n_1 = int(MY_ID[-1]) % 5\n",
    "    n_2 = int(MY_ID[-2]) % 5\n",
    "    n_3 = int(MY_ID[-3]) % 5\n",
    "    \n",
    "    while n_1 == n_2 or n_1 == n_3:\n",
    "        n_1 = (n_1 + 1) % 5\n",
    "    \n",
    "    while n_1 == n_2 or n_2 == n_3:\n",
    "        n_2 = (n_2 + 1) % 5\n",
    "        \n",
    "except Exception as e:\n",
    "    print('Please enter a valid ID...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "kub1WgPZjwwF"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/My Drive/XXXXXX/corpus-20090418' \n",
    "# Edit this path if the data directory is not in the current directory\n",
    "\n",
    "try:\n",
    "    os.makedirs('Data_Sample')\n",
    "    os.makedirs('Original_Sample')\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "for i in range(3):\n",
    "    task = task_dict[id_dict[i]]\n",
    "    \n",
    "    for file in os.listdir(data_path):\n",
    "        if task in file:\n",
    "            if 'orig' in file:\n",
    "                copyfile(data_path + '/' + file, 'Original_Sample/' + file)\n",
    "            else:\n",
    "                copyfile(data_path + '/' + file, 'Data_Sample/' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5gKWeWFwamK"
   },
   "source": [
    "###Finding the number of docs created in Data_Sample and Original_Sample folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "zaBhy-8gWp-s",
    "outputId": "0a5fb183-cbef-4377-e202-0d0948295729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents created in Data_Sample folder :  19\n",
      "Number of documents created in Original_Sample folder :  1\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "q=0\n",
    "for filename in os.listdir(\"/content/Data_Sample\"):\n",
    "    answerdoc=os.path.join(\"/content/Data_Sample\",filename)\n",
    "    p+=1\n",
    "for filename in os.listdir(\"/content/Original_Sample\"):\n",
    "    answerdoc=os.path.join(\"/content/Original_Sample\",filename)\n",
    "    q+=1\n",
    "print(\"Number of documents created in Data_Sample folder : \", str(p))\n",
    "print(\"Number of documents created in Original_Sample folder : \", str(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "jXZ0VqAdV-6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn5USwF5jwwH"
   },
   "source": [
    "Your dataset for this project will be in <b>Data_Sample</b>\n",
    "\n",
    "\n",
    "Your query choices will be in <b>Original_Sample</b> directory\n",
    "\n",
    "\n",
    "You have to use any one original Wikipedia article from <b>Original_Sample</b> for <b>Fact Check</b> steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "LzO_WLiijwwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtOdn8W-jwwK"
   },
   "source": [
    "### STEP - 1: Shingling (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "pPZHu5qSjwwK"
   },
   "outputs": [],
   "source": [
    "# Type your code here... \n",
    "# Create necessary number of cells below this cell\n",
    "# k = 3\n",
    "\n",
    "# NOTE: No complex text processing is required\n",
    "# convert just upper case characters to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "SZFxIGs0tTxj"
   },
   "outputs": [],
   "source": [
    "samplefolder_3shingles=set()\n",
    "samplefolder_4shingles=set()\n",
    "samplefolder_5shingles=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "ELjMkYUauGrk"
   },
   "outputs": [],
   "source": [
    "def task1(folder_path):\n",
    "  for filename in os.listdir(folder_path):\n",
    "    answerdoc=os.path.join(folder_path,filename)\n",
    "    f = open(answerdoc, \"r\",encoding='utf-8',errors='ignore')\n",
    "    answer=f.read()\n",
    "    tokens=answer.split()\n",
    "  # Creating unique 3-shingles\n",
    "    k=3\n",
    "    for i in range(0,len(tokens)-k+1):\n",
    "      samplefolder_3shingles.add(str(tokens[i]+\" \"+tokens[i+1]+\" \"+tokens[i+2]).lower())\n",
    "  # Creating unique 4-shingles\n",
    "    k=4\n",
    "    for i in range(0,len(tokens)-k+1):\n",
    "      samplefolder_4shingles.add(str(tokens[i]+\" \"+tokens[i+1]+\" \"+tokens[i+2]+\" \"+tokens[i+3]).lower())\n",
    "  # Creating unique 5-shingles\n",
    "    k=5\n",
    "    for i in range(0,len(tokens)-k+1):\n",
    "      samplefolder_5shingles.add(str(tokens[i]+\" \"+tokens[i+1]+\" \"+tokens[i+2]+\" \"+tokens[i+3]+\" \"+tokens[i+4]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "Afjh7OxXuyms"
   },
   "outputs": [],
   "source": [
    "task1('/content/Data_Sample')\n",
    "task1('/content/Original_Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "gdg141SoOycq",
    "outputId": "1afb39bc-d149-4eca-fc47-c57d02a8f528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 3-shingles: 3133\n",
      "Number of unique 4-shingles: 3347\n",
      "Number of unique 5-shingles: 3439\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique 3-shingles:' ,len(samplefolder_3shingles))\n",
    "print('Number of unique 4-shingles:' ,len(samplefolder_4shingles))\n",
    "print('Number of unique 5-shingles:' ,len(samplefolder_5shingles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm9ZNA_wjwwP"
   },
   "source": [
    "Report results (number of unique k-shingles) for k={3,4,5} below:\n",
    "1. k=3: 3133\n",
    "2. k=4: 3347\n",
    "3. k=5: 3439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "4NbL1hvEjwwP"
   },
   "outputs": [],
   "source": [
    "# Type your code to get the 5-shingle index here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "6mLbI6hkLqU-"
   },
   "outputs": [],
   "source": [
    "# Creating 5-shingle index. In the data sample folder, for each doc, 5-shingles are created and stored in eachdoc_5shingles list. All these lists are added into\n",
    "# fiveShingles_index dictionary with key as the doc name and value as its corresponding 5-shingles ascii values.\n",
    "# So, fiveShingles_index length is nothing but number of documents in the sample folder.\n",
    "# The value for each key in fiveShingles_index dictonary is 5shingles and length of that value is the number of unique shingles in that particular document.\n",
    "def generate_shingles(filepath,k):\n",
    "  fiveShingles_index={}\n",
    "  for filename in os.listdir(filepath):\n",
    "    answerdoc=os.path.join(filepath,filename)\n",
    "    f = open(answerdoc, \"r\",encoding='utf-8',errors='ignore')\n",
    "    answer=f.read()\n",
    "    tokens=answer.split()\n",
    "    eachdoc_5shingles=[]\n",
    "    for i in range(0,len(tokens)-k+1):\n",
    "      c=str(tokens[i]+\" \"+tokens[i+1]+\" \"+tokens[i+2]+\" \"+tokens[i+3]+\" \"+tokens[i+4]).lower()\n",
    "      c=binascii.crc32(bytes(c,'utf-8'))\n",
    "      if c not in eachdoc_5shingles:\n",
    "        eachdoc_5shingles.append(c)\n",
    "    fiveShingles_index[filename]=eachdoc_5shingles\n",
    "  return fiveShingles_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "HFQrM8PwSNeq"
   },
   "outputs": [],
   "source": [
    "data_Sample_5shingleindex=generate_shingles('/content/Data_Sample',5)\n",
    "Original_Sample_5shingleindex=generate_shingles('/content/Original_Sample',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "RnSJ6S88S835",
    "outputId": "9286fd1a-59b9-4235-a6bf-eb61581c1637",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing 5 shingles ascii values for each doc in Data_Sample folder\n",
    "#for k, v in data_Sample_5shingleindex.items():\n",
    "  #print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "bbvj5rjVTuCN",
    "outputId": "a5a6cd9e-9187-4f63-97f7-fe29e891e743"
   },
   "outputs": [],
   "source": [
    "# Printing 5 shingles ascii values for each doc in Original_Sample folder\n",
    "#for k, v in Original_Sample_5shingleindex.items():\n",
    "  #print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "3P4SNYNJT00d"
   },
   "outputs": [],
   "source": [
    "#Getting the count of unique 5 shingles from Data_sample folder and Orig_Sample folder\n",
    "all_unique_5shingles_index=set()\n",
    "for k, v in data_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)\n",
    "for k, v in Original_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qAtoOmfKUikH",
    "outputId": "d18e08df-c187-4f74-a276-16a3032a11a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3439"
      ]
     },
     "execution_count": 235,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unique_5shingles_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dx_AMCG6Vp3_",
    "outputId": "f6e7e757-9e77-40ea-afde-7e59f1f5412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4142\n"
     ]
    }
   ],
   "source": [
    "# Just a cross check for my understanding\n",
    "# Here, I am checking the total number of unique shingles from each doc. This is equal to 4142. This is different than the number \n",
    "# of unique shingles in all the documents, which is 3439 as calculated in one of the above cells. This means some shingles are\n",
    "# common even across the different documents. Hence 3439(count of unique shingles from all docs) < 4142( sum of unique shingles from each doc)\n",
    "t=0\n",
    "for k, v in data_Sample_5shingleindex.items():\n",
    "  #print(len(v))\n",
    "  t+=len(v)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycPAPDNnjwwU"
   },
   "source": [
    "### STEP - 2: Min-Hashing (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "v3RI4JAcjwwU"
   },
   "outputs": [],
   "source": [
    "# ***************************************************NEW***********************************************************\n",
    "# Generate Hash functions - \n",
    "    # We use (ax + b) mod N formula to permute shingle index\n",
    "    # where a,b are random numbers, N total index size, and x is the index\n",
    "# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes\n",
    "# Following function takes total index size N and L as arguments\n",
    "    # And returns L new lists of size N\n",
    "    \n",
    "def get_hash_functions(N,L):\n",
    "    hash_functions = []\n",
    "    \n",
    "    for itr in range(L):\n",
    "        a=randrange(1,400)\n",
    "        b=randrange(1,400)\n",
    "        \n",
    "        new_hash_function = []\n",
    "        for i in range(N):\n",
    "            new_hash_function.append((a * i + b) % N)\n",
    "        \n",
    "        hash_functions.append(new_hash_function)\n",
    "    return hash_functions\n",
    "        \n",
    "# test\n",
    "# hash_functions = get_hash_functions(3439,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "DRPuTRkuR28N"
   },
   "outputs": [],
   "source": [
    "L= [50,100,200,500,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "rqSSBrzwjwwW"
   },
   "outputs": [],
   "source": [
    "# Type your code here to generate all L hash functions\n",
    "# Generate hash functions only for shingle index created for k=5\n",
    "#Generating a\n",
    "h_f=[]\n",
    "for i in L:\n",
    "  hash_functions=get_hash_functions(len(all_unique_5shingles_index),i)\n",
    "  h_f.append(hash_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "4fMCaqWfiY2d",
    "outputId": "4b3589cf-cab0-4175-c47f-6f0afd2f1363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of L's: 5\n",
      "Number of hash functions in each of the 5 L's:\n",
      "50\n",
      "100\n",
      "200\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of L's:\",str(len(h_f)))\n",
    "print(\"Number of hash functions in each of the 5 L's:\")\n",
    "for i in range(0,len(h_f)):\n",
    "  print(len(h_f[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "gIkS-I6djwwb"
   },
   "outputs": [],
   "source": [
    "# Type your code here to get the final signature matrix S here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "T7d3PgZpatYz"
   },
   "outputs": [],
   "source": [
    "def generate_signaturematrix(docsshingles_index,allshingles_index,hash_functions,L):\n",
    "  all=[]\n",
    "  #docs=[]\n",
    "  for k,v in docsshingles_index.items():\n",
    "    middle=[np.inf for r in [None]*L]\n",
    "    #for i in range(0,3439):\n",
    "    for m in allshingles_index:\n",
    "      if (m in v):\n",
    "        i=list(allshingles_index).index(m)\n",
    "        for j in range(0,len(hash_functions)):\n",
    "          if hash_functions[j][i]<middle[j]:\n",
    "            middle[j]=hash_functions[j][i]\n",
    "            #print(middle) #use this print to check\n",
    "    #docs.append(k)\n",
    "    all.append(middle)\n",
    "  return np.array(all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "EbDDrQMKc_dO"
   },
   "outputs": [],
   "source": [
    "# Reporting signature matrices for each L hash fucntions where L=[50,100,200,500,1000]\n",
    "all_orig_sign_matrices=[]\n",
    "all_data_sign_matrices=[]\n",
    "for z in range(0,len(h_f)):\n",
    "  orig_sample_signmatrix=generate_signaturematrix(Original_Sample_5shingleindex,all_unique_5shingles_index,h_f[z],len(h_f[z]))\n",
    "  data_sample_signmatrix=generate_signaturematrix(data_Sample_5shingleindex,all_unique_5shingles_index,h_f[z],len(h_f[z]))\n",
    "  all_orig_sign_matrices.append(orig_sample_signmatrix)\n",
    "  all_data_sign_matrices.append(data_sample_signmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "0Xxs0YA3r6gG",
    "outputId": "2bfce75b-fedb-46af-cf79-d735e79aa89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample folder signature matrices size for L = 50 : (50, 19)\n",
      "Original Sample folder signature matrices size for L = 50 : (50, 1)\n",
      "Data Sample folder signature matrices size for L = 100 : (100, 19)\n",
      "Original Sample folder signature matrices size for L = 100 : (100, 1)\n",
      "Data Sample folder signature matrices size for L = 200 : (200, 19)\n",
      "Original Sample folder signature matrices size for L = 200 : (200, 1)\n",
      "Data Sample folder signature matrices size for L = 500 : (500, 19)\n",
      "Original Sample folder signature matrices size for L = 500 : (500, 1)\n",
      "Data Sample folder signature matrices size for L = 1000 : (1000, 19)\n",
      "Original Sample folder signature matrices size for L = 1000 : (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(all_data_sign_matrices)):\n",
    "  # all_data_sign_matrices is a list of signature matrices for L=[50,100,200,500,1000] respectively\n",
    "  print(\"Data Sample folder signature matrices size for L =\",str(len(all_data_sign_matrices[i])),\":\",str(all_data_sign_matrices[i].shape))\n",
    "  # all_orig_sign_matrices is a list of signature matrices for L=[50,100,200,500,1000] respectively\n",
    "  print(\"Original Sample folder signature matrices size for L =\",str(len(all_orig_sign_matrices[i])),\":\",str(all_orig_sign_matrices[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ou75CsuueuAl",
    "outputId": "51ce3611-a656-4d83-cb63-4ae29f87746a"
   },
   "outputs": [],
   "source": [
    "# Printing signature matrices for all L=50 for data and original sample files. As its lenghty to print, \n",
    "# printing only for L=50, To get for other L's\n",
    "# the index can be cahnegd\n",
    "#print(all_data_sign_matrices[0])\n",
    "#print(all_orig_sign_matrices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LOjGIaEVVWI"
   },
   "source": [
    "####Calculcating Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "F3oyLtaOjS-Y"
   },
   "outputs": [],
   "source": [
    "# Function to get the jaccard similarity. I used this code avaialble on the internet.\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return round(float(intersection) / union,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "rNd4TnnV6Ey4"
   },
   "outputs": [],
   "source": [
    "def generate_jaccard_similarity(orig_signature_matrix,data_signature_matrix):\n",
    "  all_sim=[]\n",
    "  for i in range(0,5):\n",
    "    sim=[]\n",
    "    blue=orig_signature_matrix[i][:,0].tolist()\n",
    "    for j in range(0,p):\n",
    "      green=data_signature_matrix[i][:,j].tolist()\n",
    "      sim.append(jaccard(blue,green))\n",
    "    all_sim.append(sim)\n",
    "  jsim_all=[]\n",
    "  for m in range(0,len(all_sim)):\n",
    "    j_sim={}\n",
    "    for i in range(0,p):\n",
    "      j_sim[doc_names[i]]=all_sim[m][i]\n",
    "    jsim_all.append(j_sim)\n",
    "  return jsim_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "J5CiZYoXmakv"
   },
   "outputs": [],
   "source": [
    "#Here, jaccard similarity for all 19 docs with respect to original is calculated.\n",
    "# This is performed across asll 5 L's\n",
    "# The resulting all_sim is a list of 5 elements representing L=[50,100,200,500,1000] resp\n",
    "# Each element in jsim_all is a dictionary with keys as doc names and values as its jaccard similarities with original\n",
    "all_sim=[]\n",
    "for i in range(0,len(L)):\n",
    "  sim=[]\n",
    "  blue=all_orig_sign_matrices[i][:,0].tolist()\n",
    "  for j in range(0,p):\n",
    "    green=all_data_sign_matrices[i][:,j].tolist()\n",
    "    sim.append(jaccard(blue,green))\n",
    "  all_sim.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "yKYL8HIeo6T3"
   },
   "outputs": [],
   "source": [
    "doc_names=[]\n",
    "for k in data_Sample_5shingleindex.keys():\n",
    "  doc_names.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "5ubFUyxdp8HI"
   },
   "outputs": [],
   "source": [
    "jsim_all=[]\n",
    "for m in range(0,len(all_sim)):\n",
    "  j_sim={}\n",
    "  for i in range(0,p):\n",
    "    j_sim[doc_names[i]]=all_sim[m][i]\n",
    "  jsim_all.append(j_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "UgqUK9TVq7Qb",
    "outputId": "6638d7ea-d0c6-430b-b304-e4b97558c178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g0pA_taska.txt': 0.0288,\n",
       " 'g0pB_taska.txt': 0.0277,\n",
       " 'g0pC_taska.txt': 0.0293,\n",
       " 'g0pD_taska.txt': 0.0299,\n",
       " 'g0pE_taska.txt': 0.0293,\n",
       " 'g1pA_taska.txt': 0.0293,\n",
       " 'g1pB_taska.txt': 0.0299,\n",
       " 'g1pD_taska.txt': 0.0299,\n",
       " 'g2pA_taska.txt': 0.0283,\n",
       " 'g2pB_taska.txt': 0.0293,\n",
       " 'g2pC_taska.txt': 0.0293,\n",
       " 'g2pE_taska.txt': 0.0262,\n",
       " 'g3pA_taska.txt': 0.0299,\n",
       " 'g3pB_taska.txt': 0.0299,\n",
       " 'g3pC_taska.txt': 0.0293,\n",
       " 'g4pB_taska.txt': 0.0293,\n",
       " 'g4pC_taska.txt': 0.0283,\n",
       " 'g4pD_taska.txt': 0.0299,\n",
       " 'g4pE_taska.txt': 0.0293}"
      ]
     },
     "execution_count": 253,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each element in jsim_all has jaccard similartoes for each of L=[50,100,200,500,1000]. Here, I printed for L=1000\n",
    "# Fact Check\n",
    "jsim_all[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNJV_QdJKtYs"
   },
   "source": [
    "**Entire code for creating 5 shingles index, generating signature matrix for each of the L=[50,100,200,500,1000] and calculating jaccard similarities-fact check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "HBjz37lqjwwf",
    "outputId": "edc509f8-5652-449f-a4ab-d29ed8961ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************  Jaccard Similarities for L=[50,100,200,500,1000] without any threshold*******************************************\n",
      "[{'g3pA_taska.txt': 0.1765, 'g0pB_taska.txt': 0.1905, 'g4pB_taska.txt': 0.1905, 'g0pD_taska.txt': 0.1765, 'g1pA_taska.txt': 0.1494, 'g1pD_taska.txt': 0.1905, 'g0pC_taska.txt': 0.1905, 'g4pC_taska.txt': 0.2195, 'g0pE_taska.txt': 0.2346, 'g2pC_taska.txt': 0.1905, 'g3pC_taska.txt': 0.1765, 'g2pB_taska.txt': 0.1765, 'g0pA_taska.txt': 0.2048, 'g4pD_taska.txt': 0.1628, 'g2pE_taska.txt': 0.1628, 'g1pB_taska.txt': 0.2195, 'g2pA_taska.txt': 0.2048, 'g4pE_taska.txt': 0.2048, 'g3pB_taska.txt': 0.1905}, {'g3pA_taska.txt': 0.1429, 'g0pB_taska.txt': 0.1429, 'g4pB_taska.txt': 0.1494, 'g0pD_taska.txt': 0.1494, 'g1pA_taska.txt': 0.1494, 'g1pD_taska.txt': 0.1429, 'g0pC_taska.txt': 0.1299, 'g4pC_taska.txt': 0.1696, 'g0pE_taska.txt': 0.1561, 'g2pC_taska.txt': 0.1299, 'g3pC_taska.txt': 0.1429, 'g2pB_taska.txt': 0.1364, 'g0pA_taska.txt': 0.1364, 'g4pD_taska.txt': 0.1494, 'g2pE_taska.txt': 0.1494, 'g1pB_taska.txt': 0.1628, 'g2pA_taska.txt': 0.1364, 'g4pE_taska.txt': 0.1364, 'g3pB_taska.txt': 0.1364}, {'g3pA_taska.txt': 0.0959, 'g0pB_taska.txt': 0.087, 'g4pB_taska.txt': 0.0929, 'g0pD_taska.txt': 0.0959, 'g1pA_taska.txt': 0.0899, 'g1pD_taska.txt': 0.0989, 'g0pC_taska.txt': 0.087, 'g4pC_taska.txt': 0.0899, 'g0pE_taska.txt': 0.1019, 'g2pC_taska.txt': 0.0989, 'g3pC_taska.txt': 0.0959, 'g2pB_taska.txt': 0.0899, 'g0pA_taska.txt': 0.084, 'g4pD_taska.txt': 0.087, 'g2pE_taska.txt': 0.0782, 'g1pB_taska.txt': 0.0989, 'g2pA_taska.txt': 0.087, 'g4pE_taska.txt': 0.0899, 'g3pB_taska.txt': 0.0959}, {'g3pA_taska.txt': 0.0471, 'g0pB_taska.txt': 0.0438, 'g4pB_taska.txt': 0.046, 'g0pD_taska.txt': 0.0493, 'g1pA_taska.txt': 0.0471, 'g1pD_taska.txt': 0.046, 'g0pC_taska.txt': 0.0493, 'g4pC_taska.txt': 0.0504, 'g0pE_taska.txt': 0.0482, 'g2pC_taska.txt': 0.0482, 'g3pC_taska.txt': 0.0504, 'g2pB_taska.txt': 0.0493, 'g0pA_taska.txt': 0.0493, 'g4pD_taska.txt': 0.0493, 'g2pE_taska.txt': 0.0417, 'g1pB_taska.txt': 0.0493, 'g2pA_taska.txt': 0.046, 'g4pE_taska.txt': 0.0449, 'g3pB_taska.txt': 0.046}, {'g3pA_taska.txt': 0.0288, 'g0pB_taska.txt': 0.0267, 'g4pB_taska.txt': 0.0283, 'g0pD_taska.txt': 0.0288, 'g1pA_taska.txt': 0.0283, 'g1pD_taska.txt': 0.0288, 'g0pC_taska.txt': 0.0283, 'g4pC_taska.txt': 0.0288, 'g0pE_taska.txt': 0.0283, 'g2pC_taska.txt': 0.0277, 'g3pC_taska.txt': 0.0288, 'g2pB_taska.txt': 0.0272, 'g0pA_taska.txt': 0.0277, 'g4pD_taska.txt': 0.0283, 'g2pE_taska.txt': 0.0246, 'g1pB_taska.txt': 0.0288, 'g2pA_taska.txt': 0.0267, 'g4pE_taska.txt': 0.0283, 'g3pB_taska.txt': 0.0288}]\n",
      "************************* Jaccard Similarities for L=[50,100,200,500,1000] greater than t=0.2 **************************\n",
      "[{'g4pC_taska.txt': 0.2195, 'g0pE_taska.txt': 0.2346, 'g0pA_taska.txt': 0.2048, 'g1pB_taska.txt': 0.2195, 'g2pA_taska.txt': 0.2048, 'g4pE_taska.txt': 0.2048}, {}, {}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "# Type your code here to do the fact check \n",
    "#      with any one query document in the 'Original_Sample' directory\n",
    "\n",
    "original = 'Original_Sample/orig_taskc.txt'\n",
    "\n",
    "# STEP-1: Generate 5-shingles \n",
    "    # (if any shingles are not present in your shingle index, simply ignore them)\n",
    "data_Sample_5shingleindex=generate_shingles('/content/Data_Sample',5)\n",
    "Original_Sample_5shingleindex=generate_shingles('/content/Original_Sample',5)\n",
    "    \n",
    "all_unique_5shingles_index=set()\n",
    "for k, v in data_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)\n",
    "for k, v in Original_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)    \n",
    "    \n",
    "# STEP-2: Generate signature vector from L hash functions\n",
    "L=[50,100,200,500,1000]\n",
    "\n",
    "h_f=[]\n",
    "for i in L:\n",
    "  hash_functions=get_hash_functions(len(all_unique_5shingles_index),i)\n",
    "  h_f.append(hash_functions)\n",
    "\n",
    "all_orig_sign_matrices=[]\n",
    "all_data_sign_matrices=[]\n",
    "for z in range(0,len(h_f)):\n",
    "  orig_sample_signmatrix=generate_signaturematrix(Original_Sample_5shingleindex,all_unique_5shingles_index,h_f[z],len(h_f[z]))\n",
    "  data_sample_signmatrix=generate_signaturematrix(data_Sample_5shingleindex,all_unique_5shingles_index,h_f[z],len(h_f[z]))\n",
    "  all_orig_sign_matrices.append(orig_sample_signmatrix)\n",
    "  all_data_sign_matrices.append(data_sample_signmatrix)\n",
    "\n",
    "doc_names=[]\n",
    "for k in data_Sample_5shingleindex.keys():\n",
    "  doc_names.append(k)\n",
    "\n",
    "# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n",
    "    # and all other documents \n",
    "jaccard_similarities=generate_jaccard_similarity(all_orig_sign_matrices,all_data_sign_matrices)\n",
    "print(\"************************  Jaccard Similarities for L=[50,100,200,500,1000] without any threshold*******************************************\")\n",
    "print(jaccard_similarities)\n",
    "t = 0.20\n",
    "d=[]\n",
    "for e in range(0,len(jaccard_similarities)):\n",
    "  d1=[]\n",
    "  for k,v in jaccard_similarities[e].items():\n",
    "    if v<t:\n",
    "      d1.append(k)\n",
    "  d.append(d1)\n",
    "s=0\n",
    "while s<len(d):\n",
    "  #print(s)\n",
    "  for i in d[s]:\n",
    "    jaccard_similarities[s].pop(i)\n",
    "  s+=1\n",
    "print(\"************************* Jaccard Similarities for L=[50,100,200,500,1000] greater than t=0.2 **************************\")\n",
    "print(jaccard_similarities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpP4l-T0ITET"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YMA9VbGjwwh"
   },
   "source": [
    "***************************************************NEW***********************************************************\n",
    "For each L = {50,100,200,500,1000}, report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
    "Sort the documents in decreasing order of the Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "uxk9jsYwjwwh"
   },
   "outputs": [],
   "source": [
    "#This is reported above. I do not get any docs with t>0.2 except for L=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B06XGGFjwwk"
   },
   "source": [
    "### STEP - 3: LSH (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "b1cxqGy3LLIU"
   },
   "outputs": [],
   "source": [
    "#Generating signature matrix for L=1000\n",
    "data_Sample_5shingleindex=generate_shingles('/content/Data_Sample',5)\n",
    "Original_Sample_5shingleindex=generate_shingles('/content/Original_Sample',5)\n",
    "\n",
    "all_unique_5shingles_index=set()\n",
    "for k, v in data_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)\n",
    "for k, v in Original_Sample_5shingleindex.items():\n",
    "  for i in v:\n",
    "   all_unique_5shingles_index.add(i)  \n",
    "\n",
    "L=1000\n",
    "hash_functions=get_hash_functions(len(all_unique_5shingles_index),L)\n",
    "orig_sample_1000signmatrix=generate_signaturematrix(Original_Sample_5shingleindex,all_unique_5shingles_index,hash_functions,L)\n",
    "data_sample_1000signmatrix=generate_signaturematrix(data_Sample_5shingleindex,all_unique_5shingles_index,hash_functions,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "3j23Sl7-RXu6",
    "outputId": "9538a378-6e33-45ef-cac8-b43b6c0e523d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data Sample Signature matrix: (1000, 19)\n",
      "[[56 18 18 ... 18 18 56]\n",
      " [11  0 18 ... 25 14  3]\n",
      " [57 12  5 ...  0 19  8]\n",
      " ...\n",
      " [ 5  5  5 ...  5 43  5]\n",
      " [27 19 11 ... 14  7 31]\n",
      " [30 11  4 ...  0  8  1]]\n",
      "Size of Original Sample Signature Matrix: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Data Sample Signature matrix:\",str(data_sample_1000signmatrix.shape))\n",
    "print(data_sample_1000signmatrix)\n",
    "print(\"Size of Original Sample Signature Matrix:\",str(orig_sample_1000signmatrix.shape))\n",
    "#print(orig_sample_1000signmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "X-aGfsbbcBdM"
   },
   "outputs": [],
   "source": [
    "# Type your code here to hash signature matrix into B buckets\n",
    "# Use the technique to split the signature matrix into b bands of r rows\n",
    "# Convert only the signature matrix generated with L=1000\n",
    "\n",
    "b = 50\n",
    "r = 20\n",
    "B = 199\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "4fBBRmplcE24"
   },
   "outputs": [],
   "source": [
    "def generate_bands_hash(signaturematrix,bands,rowsinband,buckets):\n",
    "  allbands=[]\n",
    "  for z in range(0,bands): # 50 is number of bands\n",
    "    l=0\n",
    "    h=rowsinband\n",
    "    x=signaturematrix[l:h]\n",
    "    band=[]\n",
    "    for i in range(0,len(signaturematrix[0])):\n",
    "      zoo=random.sample(range(100),rowsinband)\n",
    "      igloo=0 \n",
    "      for j in range(0,rowsinband):\n",
    "        igloo+=x[:,i][j]*zoo[j] \n",
    "      band.append(igloo % buckets)\n",
    "    allbands.append(band)\n",
    "    l=h\n",
    "    h=h+rowsinband\n",
    "  return np.array(allbands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FveGaLmvd0LY"
   },
   "source": [
    "Generating Candidate Documents, False Positives, False Ngeatives\n",
    "Now, I have 50 bands for 19 docs and 50 bands for 1 original doc.\n",
    "Candidate docs are identified using the following criteria.\n",
    "For doc1, band1(doc1) is compared with band1(orig).\n",
    "In the 50 bands, if atleast one band value is common, which means they fall into same bucket. In this case, doc1 is a candidate document. This process is repeated over 19 docs comparing with original for all corresponding 50 bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "18FbuZ_XzyUq"
   },
   "outputs": [],
   "source": [
    "def generate_cand_fp_fn(orig_hash_sig_matrix,data_hash_sig_matrix,t):\n",
    "  hava={}\n",
    "  candidate_documents=[]\n",
    "  o=orig_hash_sig_matrix[:,0].tolist()\n",
    "  for i in range(0,p):\n",
    "    d=data_hash_sig_matrix[:,i].tolist()\n",
    "    count=0\n",
    "    for j in range(0,b):\n",
    "      if o[j]==d[j]:\n",
    "        count+=1\n",
    "    if count>=1:\n",
    "      hava[doc_names[i]]=count\n",
    "    #print(doc_names[i])\n",
    "      candidate_documents.append(doc_names[i])\n",
    "  print(\"************************* Candidate Documents ********************************\")\n",
    "  print(candidate_documents)\n",
    "  print(\"                         \")\n",
    "  print(\"************* Candidate documents and how many buckets in common with the original document ************\")\n",
    "  print(hava)\n",
    "  print(\"                         \")\n",
    "  plus={}\n",
    "  j=[]\n",
    "  o=orig_hash_sig_matrix[:,0].tolist()\n",
    "  for i in range(0,p):\n",
    "    d=data_hash_sig_matrix[:,i].tolist()\n",
    "    j.append(jaccard(o,d))\n",
    "    plus[doc_names[i]]=jaccard(o,d)\n",
    "# Reporting jaccard similarites for candidate docs\n",
    "  print(\"************* Jaccard Similarities of Candidate Documents *****************\")\n",
    "  for k, v in plus.items():\n",
    "    if k in candidate_documents:\n",
    "      print(k,v)\n",
    "  print(\"                         \")\n",
    "  false_positives={}\n",
    "  for c in candidate_documents:\n",
    "    if plus[c]<=t:\n",
    "      false_positives[c]=plus[c]\n",
    "  print(\"                                              \")\n",
    "  print(\"****************** All documents with Jaccard Similarity >t=0.1************************\")\n",
    "  alldocgreat_jt=[]\n",
    "  for k, v in plus.items():\n",
    "    if v>t:\n",
    "     alldocgreat_jt.append(k)\n",
    "  print(alldocgreat_jt)\n",
    "  print(\"                                     \")\n",
    "  print(\"Count of False Positives:\", len(false_positives))\n",
    "  print(\"********* False positive documents and their corresponding Jaccard Similarity *************\")\n",
    "  print(false_positives)\n",
    "  print(\"                         \")\n",
    "  false_negatives={}\n",
    "  for k,v in plus.items():\n",
    "    if v>t and k not in candidate_documents:\n",
    "      false_negatives[k]=v\n",
    "  print(\"Count of False Negatives:\", len(false_negatives))\n",
    "  print(\"************ False negative documents and their corresponding Jaccard Similarity *************\")\n",
    "  print(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cdFyztau-2ir",
    "outputId": "795d1649-2a59-465b-c10b-2531b826d45c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 274,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type your code here to do generate candidate documents\n",
    "# Follow all steps from STEP - 2 fact check (except the Jaccard similarity part)\n",
    "\n",
    "# STEP - 1: Split your original document signature vector into b bands of r rows\n",
    "\n",
    "# STEP - 2: Hash using the same hash functions created for \n",
    "    # signature matrix hashing (in the previous cell)\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "orig_bands_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "LKrE3-LQAszB"
   },
   "outputs": [],
   "source": [
    "# Type your code here to do the fact check\n",
    "# Calculate Jaccard similarity of the roiginal document with only \n",
    "    # candidate documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmNMp6U3GQ4X"
   },
   "source": [
    "Report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
    "Sort the documents in decreasing order of the Jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7CpN8JFGcVe"
   },
   "source": [
    "Report the list of false positives and false negatives below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9OGFgbNGeY1"
   },
   "source": [
    "I reported **candidaite docs**, **all docs with jaccard similarity >t=0.1** and **false positives**, **false negatives** in the below cell.\n",
    "As I have only 19 docs, none of the docs have above 0.2. The max I am gettinf is 0.19. So,I am checking with t=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "rxfiRPtaA7q0",
    "outputId": "63d7610c-17c0-4fab-ca40-a3777bb20ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                             \n",
      "******************** Results for b = 50 r = 20 B = 199 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g0pC_taska.txt', 'g2pB_taska.txt', 'g1pB_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g0pC_taska.txt': 1, 'g2pB_taska.txt': 1, 'g1pB_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g0pC_taska.txt 0.0638\n",
      "g2pB_taska.txt 0.087\n",
      "g1pB_taska.txt 0.087\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "['g4pB_taska.txt', 'g1pD_taska.txt', 'g4pC_taska.txt', 'g3pC_taska.txt']\n",
      "                                     \n",
      "Count of False Positives: 3\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{'g0pC_taska.txt': 0.0638, 'g2pB_taska.txt': 0.087, 'g1pB_taska.txt': 0.087}\n",
      "                         \n",
      "Count of False Negatives: 4\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{'g4pB_taska.txt': 0.1364, 'g1pD_taska.txt': 0.1111, 'g4pC_taska.txt': 0.1111, 'g3pC_taska.txt': 0.1494}\n"
     ]
    }
   ],
   "source": [
    "b = 50\n",
    "r = 20\n",
    "B = 199\n",
    "print(\"                                                                                                             \")\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYI9xuq_LM5k"
   },
   "source": [
    "### Optimizing with different number of bands, rows and buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "33KjEQwWLZom"
   },
   "outputs": [],
   "source": [
    "# Experiment with different values of b,r,B, and t\n",
    "    # to reduce the number of false positives and false negatives\n",
    "    # Report all results in a table in a separate word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "yTZxjMfuDHDA",
    "outputId": "1c1fbc28-d100-478b-a1f6-f4ea79dbb13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Results for b = 40 r = 25 B = 199 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g4pB_taska.txt', 'g1pB_taska.txt', 'g4pE_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g4pB_taska.txt': 1, 'g1pB_taska.txt': 1, 'g4pE_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g4pB_taska.txt 0.0667\n",
      "g1pB_taska.txt 0.1111\n",
      "g4pE_taska.txt 0.1111\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "['g0pD_taska.txt', 'g2pB_taska.txt', 'g2pE_taska.txt', 'g1pB_taska.txt', 'g4pE_taska.txt', 'g3pB_taska.txt']\n",
      "                                     \n",
      "Count of False Positives: 1\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{'g4pB_taska.txt': 0.0667}\n",
      "                         \n",
      "Count of False Negatives: 4\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{'g0pD_taska.txt': 0.1111, 'g2pB_taska.txt': 0.1268, 'g2pE_taska.txt': 0.1268, 'g3pB_taska.txt': 0.1111}\n"
     ]
    }
   ],
   "source": [
    "b = 40\n",
    "r = 25\n",
    "B = 199\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "975o6luWDG_f",
    "outputId": "d0f75b14-b31c-4b0c-b907-e0834b2e5804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Results for b = 10 r = 100 B = 99 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g0pC_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g0pC_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g0pC_taska.txt 0.0526\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "['g0pD_taska.txt', 'g0pE_taska.txt', 'g0pA_taska.txt']\n",
      "                                     \n",
      "Count of False Positives: 1\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{'g0pC_taska.txt': 0.0526}\n",
      "                         \n",
      "Count of False Negatives: 3\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{'g0pD_taska.txt': 0.1111, 'g0pE_taska.txt': 0.1765, 'g0pA_taska.txt': 0.1111}\n"
     ]
    }
   ],
   "source": [
    "b = 10\n",
    "r = 100\n",
    "B = 99\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "w33sadtRDG8c",
    "outputId": "d50dda98-eb5b-4a94-b859-d4deec0342fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Results for b = 125 r = 8 B = 300 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g4pB_taska.txt', 'g0pD_taska.txt', 'g1pA_taska.txt', 'g3pC_taska.txt', 'g0pA_taska.txt', 'g4pD_taska.txt', 'g3pB_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g4pB_taska.txt': 1, 'g0pD_taska.txt': 1, 'g1pA_taska.txt': 1, 'g3pC_taska.txt': 1, 'g0pA_taska.txt': 1, 'g4pD_taska.txt': 2, 'g3pB_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g4pB_taska.txt 0.1364\n",
      "g0pD_taska.txt 0.1364\n",
      "g1pA_taska.txt 0.1628\n",
      "g3pC_taska.txt 0.1468\n",
      "g0pA_taska.txt 0.1312\n",
      "g4pD_taska.txt 0.1792\n",
      "g3pB_taska.txt 0.1211\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "['g3pA_taska.txt', 'g0pB_taska.txt', 'g4pB_taska.txt', 'g0pD_taska.txt', 'g1pA_taska.txt', 'g1pD_taska.txt', 'g0pC_taska.txt', 'g4pC_taska.txt', 'g0pE_taska.txt', 'g2pC_taska.txt', 'g3pC_taska.txt', 'g2pB_taska.txt', 'g0pA_taska.txt', 'g4pD_taska.txt', 'g2pE_taska.txt', 'g1pB_taska.txt', 'g2pA_taska.txt', 'g4pE_taska.txt', 'g3pB_taska.txt']\n",
      "                                     \n",
      "Count of False Positives: 0\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{}\n",
      "                         \n",
      "Count of False Negatives: 12\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{'g3pA_taska.txt': 0.1682, 'g0pB_taska.txt': 0.1848, 'g1pD_taska.txt': 0.1682, 'g0pC_taska.txt': 0.1574, 'g4pC_taska.txt': 0.1521, 'g0pE_taska.txt': 0.1161, 'g2pC_taska.txt': 0.1416, 'g2pB_taska.txt': 0.1416, 'g2pE_taska.txt': 0.1312, 'g1pB_taska.txt': 0.1416, 'g2pA_taska.txt': 0.1364, 'g4pE_taska.txt': 0.1468}\n"
     ]
    }
   ],
   "source": [
    "b = 125\n",
    "r = 8\n",
    "B = 300\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "NbRgxNiwDG4g",
    "outputId": "0dec7ba5-2ed6-4702-fe42-f6be46bb91b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Results for b = 25 r = 40 B = 300 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g4pD_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g4pD_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g4pD_taska.txt 0.0204\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "[]\n",
      "                                     \n",
      "Count of False Positives: 1\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{'g4pD_taska.txt': 0.0204}\n",
      "                         \n",
      "Count of False Negatives: 0\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "b = 25\n",
    "r = 40\n",
    "B = 300\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "fTahnhaJDG0z",
    "outputId": "2a82a5da-0da9-43da-f302-baaa79515352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Results for b = 100 r = 10 B = 1000 **********************************\n",
      "                                                                                                           \n",
      "************************* Candidate Documents ********************************\n",
      "['g2pC_taska.txt', 'g4pD_taska.txt', 'g1pB_taska.txt', 'g4pE_taska.txt']\n",
      "                         \n",
      "************* Candidate documents and how many buckets in common with the original document ************\n",
      "{'g2pC_taska.txt': 1, 'g4pD_taska.txt': 1, 'g1pB_taska.txt': 1, 'g4pE_taska.txt': 1}\n",
      "                         \n",
      "************* Jaccard Similarities of Candidate Documents *****************\n",
      "g2pC_taska.txt 0.0471\n",
      "g4pD_taska.txt 0.0526\n",
      "g1pB_taska.txt 0.0471\n",
      "g4pE_taska.txt 0.0417\n",
      "                         \n",
      "                                              \n",
      "****************** All documents with Jaccard Similarity >t=0.1************************\n",
      "[]\n",
      "                                     \n",
      "Count of False Positives: 4\n",
      "********* False positive documents and their corresponding Jaccard Similarity *************\n",
      "{'g2pC_taska.txt': 0.0471, 'g4pD_taska.txt': 0.0526, 'g1pB_taska.txt': 0.0471, 'g4pE_taska.txt': 0.0417}\n",
      "                         \n",
      "Count of False Negatives: 0\n",
      "************ False negative documents and their corresponding Jaccard Similarity *************\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "b = 100\n",
    "r = 10\n",
    "B = 1000\n",
    "print(\"******************** Results for b =\", str(b),\"r =\", str(r), \"B =\", str(B),\"**********************************\")\n",
    "print(\"                                                                                                           \")\n",
    "orig_bands_hash=generate_bands_hash(orig_sample_1000signmatrix,b,r,B)\n",
    "data_bands_hash=generate_bands_hash(data_sample_1000signmatrix,b,r,B)\n",
    "t=0.1\n",
    "generate_cand_fp_fn(orig_bands_hash,data_bands_hash,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x2G7Kg8DGwf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0soTwpE-Wc5"
   },
   "source": [
    "#**************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
